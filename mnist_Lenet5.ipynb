{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784) (50000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "DATA_PATH = Path(\"data\")\n",
    "PATH = DATA_PATH / \"mnist\"\n",
    "\n",
    "PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FILENAME = \"mnist.pkl.gz\"\n",
    "\n",
    "with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\n",
    "            ((x_train, y_train), (x_valid, y_valid),\n",
    "             _) = pickle.load(f, encoding=\"latin-1\")\n",
    "print(x_train.shape, y_train.shape, x_valid.shape, y_valid.shape)\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = torch.reshape(torch.from_numpy(x), (len(y), 1, 28, 28))\n",
    "        self.y = torch.from_numpy(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # N(batch), Channels ,Height, Width\n",
    "        return self.x[i], self.y[i]\n",
    "\n",
    "train_mnist_dataset = MnistDataset(x_train, y_train)\n",
    "test_mnist_dataset = MnistDataset(x_valid, y_valid)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_dataloader = DataLoader(\n",
    "    train_mnist_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(\n",
    "    test_mnist_dataset, batch_size=len(x_valid), shuffle=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# x_batch, y_batch = next(iter(train_dataloader))\n",
    "# plt.figure(num='Training set samples',figsize=(8,8))\n",
    "# for i in range(4):\n",
    "#     plt.subplot(2,2,i+1)     \n",
    "#     plt.title(y_batch[i])   \n",
    "#     plt.imshow(x_batch[i][0], plt.cm.gray)      \n",
    "#     plt.axis('off')     \n",
    "# plt.show()   #显示窗口\n",
    "\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=6,\n",
    "                               kernel_size=5, stride=1, padding=0)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(\n",
    "            in_channels=6, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(in_features=4*4*16, out_features=120, bias=True)\n",
    "        self.fc2 = nn.Linear(in_features=120, out_features=84, bias=True)\n",
    "        self.fc3 = nn.Linear(in_features=84, out_features=10, bias=True)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = nn.functional.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(-1, 4*4*16)  # should be 5*5*16 if it's 32*32 image\n",
    "        x = nn.functional.relu(self.fc1(x))\n",
    "        x = nn.functional.relu(self.fc2(x))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# define Lenet-5 model\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "def accuracy_on_test(model, onGPU=False):\n",
    "    with torch.no_grad():\n",
    "        x_batch, y_batch = next(iter(test_dataloader))\n",
    "        if onGPU:\n",
    "            x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "        y_output = model(x_batch)\n",
    "        y_predict = torch.max(y_output, 1).indices\n",
    "        correct_nums = (y_predict == y_batch).sum().item()\n",
    "        return correct_nums, len(y_batch), correct_nums/len(y_batch) * 100\n",
    "\n",
    "\n",
    "def train(model, optimizer, onGPU=False):\n",
    "    start = time.time()\n",
    "    for epoch in range(EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        for x_batch, y_batch in iter(train_dataloader):\n",
    "            if onGPU:\n",
    "                x_batch, y_batch = x_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward\n",
    "            y_output = model(x_batch)\n",
    "\n",
    "            # compute loss\n",
    "            loss = criterion(y_output, y_batch)\n",
    "\n",
    "            # compute gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # optimize\n",
    "            optimizer.step()\n",
    "\n",
    "            # add loss\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        correct, total, accuracy = accuracy_on_test(model, onGPU)\n",
    "        print('Epoch: %d, loss: %.3f, accuracy on test set:  %d/%d = %.3f' %\n",
    "              (epoch + 1, epoch_loss, correct, total, accuracy))\n",
    "\n",
    "    end = time.time()-start\n",
    "    if onGPU:\n",
    "        print('Finished GPU Training,takes %d seconds\\n' % (end))\n",
    "    else:\n",
    "        print('Finished CPU Training,takes %d seconds\\n' % (end))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, loss: 2302.097, accuracy on test set:  1065/10000 = 10.650\n",
      "Epoch: 2, loss: 2299.641, accuracy on test set:  1805/10000 = 18.050\n",
      "Epoch: 3, loss: 2084.660, accuracy on test set:  7731/10000 = 77.310\n",
      "Epoch: 4, loss: 1640.802, accuracy on test set:  8937/10000 = 89.370\n",
      "Epoch: 5, loss: 1560.787, accuracy on test set:  9339/10000 = 93.390\n",
      "Epoch: 6, loss: 1536.658, accuracy on test set:  9456/10000 = 94.560\n",
      "Epoch: 7, loss: 1522.804, accuracy on test set:  9533/10000 = 95.330\n",
      "Epoch: 8, loss: 1513.329, accuracy on test set:  9594/10000 = 95.940\n",
      "Epoch: 9, loss: 1506.582, accuracy on test set:  9612/10000 = 96.120\n",
      "Epoch: 10, loss: 1501.107, accuracy on test set:  9638/10000 = 96.380\n",
      "Epoch: 11, loss: 1497.258, accuracy on test set:  9704/10000 = 97.040\n",
      "Epoch: 12, loss: 1494.349, accuracy on test set:  9694/10000 = 96.940\n",
      "Epoch: 13, loss: 1491.891, accuracy on test set:  9684/10000 = 96.840\n",
      "Epoch: 14, loss: 1489.830, accuracy on test set:  9717/10000 = 97.170\n",
      "Epoch: 15, loss: 1488.061, accuracy on test set:  9716/10000 = 97.160\n",
      "Epoch: 16, loss: 1486.518, accuracy on test set:  9760/10000 = 97.600\n",
      "Epoch: 17, loss: 1485.054, accuracy on test set:  9721/10000 = 97.210\n",
      "Epoch: 18, loss: 1484.292, accuracy on test set:  9747/10000 = 97.470\n",
      "Epoch: 19, loss: 1483.031, accuracy on test set:  9759/10000 = 97.590\n",
      "Epoch: 20, loss: 1482.375, accuracy on test set:  9738/10000 = 97.380\n",
      "Epoch: 21, loss: 1481.572, accuracy on test set:  9771/10000 = 97.710\n",
      "Epoch: 22, loss: 1480.714, accuracy on test set:  9792/10000 = 97.920\n",
      "Epoch: 23, loss: 1479.394, accuracy on test set:  9770/10000 = 97.700\n",
      "Epoch: 24, loss: 1479.412, accuracy on test set:  9798/10000 = 97.980\n",
      "Epoch: 25, loss: 1478.298, accuracy on test set:  9800/10000 = 98.000\n",
      "Epoch: 26, loss: 1477.769, accuracy on test set:  9787/10000 = 97.870\n",
      "Epoch: 27, loss: 1477.005, accuracy on test set:  9785/10000 = 97.850\n",
      "Epoch: 28, loss: 1476.556, accuracy on test set:  9818/10000 = 98.180\n",
      "Epoch: 29, loss: 1476.155, accuracy on test set:  9817/10000 = 98.170\n",
      "Epoch: 30, loss: 1475.617, accuracy on test set:  9804/10000 = 98.040\n",
      "Finished CPU Training,takes 202 seconds\n",
      "\n",
      "Epoch: 1, loss: 2302.075, accuracy on test set:  1146/10000 = 11.460\n",
      "Epoch: 2, loss: 2300.293, accuracy on test set:  2398/10000 = 23.980\n",
      "Epoch: 3, loss: 2211.540, accuracy on test set:  6325/10000 = 63.250\n",
      "Epoch: 4, loss: 1740.619, accuracy on test set:  8179/10000 = 81.790\n",
      "Epoch: 5, loss: 1648.385, accuracy on test set:  8452/10000 = 84.520\n",
      "Epoch: 6, loss: 1625.630, accuracy on test set:  8604/10000 = 86.040\n",
      "Epoch: 7, loss: 1612.184, accuracy on test set:  8671/10000 = 86.710\n",
      "Epoch: 8, loss: 1603.068, accuracy on test set:  8737/10000 = 87.370\n",
      "Epoch: 9, loss: 1596.101, accuracy on test set:  8767/10000 = 87.670\n",
      "Epoch: 10, loss: 1592.240, accuracy on test set:  8801/10000 = 88.010\n",
      "Epoch: 11, loss: 1588.922, accuracy on test set:  8825/10000 = 88.250\n",
      "Epoch: 12, loss: 1586.121, accuracy on test set:  8839/10000 = 88.390\n",
      "Epoch: 13, loss: 1584.511, accuracy on test set:  8767/10000 = 87.670\n",
      "Epoch: 14, loss: 1582.545, accuracy on test set:  8822/10000 = 88.220\n",
      "Epoch: 15, loss: 1581.351, accuracy on test set:  8856/10000 = 88.560\n",
      "Epoch: 16, loss: 1580.128, accuracy on test set:  8841/10000 = 88.410\n",
      "Epoch: 17, loss: 1530.168, accuracy on test set:  9615/10000 = 96.150\n",
      "Epoch: 18, loss: 1496.787, accuracy on test set:  9697/10000 = 96.970\n",
      "Epoch: 19, loss: 1492.328, accuracy on test set:  9722/10000 = 97.220\n",
      "Epoch: 20, loss: 1489.432, accuracy on test set:  9748/10000 = 97.480\n",
      "Epoch: 21, loss: 1487.212, accuracy on test set:  9757/10000 = 97.570\n",
      "Epoch: 22, loss: 1485.245, accuracy on test set:  9760/10000 = 97.600\n",
      "Epoch: 23, loss: 1484.442, accuracy on test set:  9769/10000 = 97.690\n",
      "Epoch: 24, loss: 1482.676, accuracy on test set:  9768/10000 = 97.680\n",
      "Epoch: 25, loss: 1481.878, accuracy on test set:  9771/10000 = 97.710\n",
      "Epoch: 26, loss: 1480.857, accuracy on test set:  9785/10000 = 97.850\n",
      "Epoch: 27, loss: 1480.134, accuracy on test set:  9797/10000 = 97.970\n",
      "Epoch: 28, loss: 1479.186, accuracy on test set:  9808/10000 = 98.080\n",
      "Epoch: 29, loss: 1478.759, accuracy on test set:  9790/10000 = 97.900\n",
      "Epoch: 30, loss: 1477.886, accuracy on test set:  9790/10000 = 97.900\n",
      "Finished GPU Training,takes 126 seconds\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define Lenet-5 model\n",
    "import time\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# start training\n",
    "EPOCHS = 30\n",
    "LEARNING_RATE = 0.03\n",
    "criterion = nn.CrossEntropyLoss()  # used in classification problem\n",
    "\n",
    "# using CPU\n",
    "model = LeNet5()\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
    "train(model, optimizer, False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "gpu_model = LeNet5()\n",
    "gpu_model = gpu_model.to(device)\n",
    "gpu_optimizer = optim.SGD(gpu_model.parameters(), lr=LEARNING_RATE)\n",
    "train(gpu_model, gpu_optimizer, True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
